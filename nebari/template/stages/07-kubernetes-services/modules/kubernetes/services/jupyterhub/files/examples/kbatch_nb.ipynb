{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba309a0-d8ff-4c18-8d94-c380e875ac46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Canonical Tests\n",
    "\n",
    "The goal of the work we performed on behalf of OGC/USGS was to enable users on QHub (and perhaps plain JupyterHub) to:\n",
    "- [ ] run long-running notebooks or scripts\n",
    "- [ ] run notebooks and scripts as cronjobs\n",
    "\n",
    "And as a stretch goal:\n",
    "- [ ] run complex workflows that require multiple steps / noteboks or scripts\n",
    "\n",
    "Additional requirements:\n",
    "- the notebook or script should work even after the user's JupyterLab session ends\n",
    "- the notebook or script can connect to the Dask-Gateway and launch a Dask cluster\n",
    "\n",
    "This notebooks will serve as a \"unit test\" for the above features, for more details [see this issue](https://github.com/Quansight/ogc-management/issues/6).\n",
    "\n",
    "## `kbatch`\n",
    "\n",
    "The first two features outline above will be handled by [`kbatch`](https://github.com/kbatch-dev/kbatch). `kbatch` consists of two major components, the frontend `kbatch` and backend `kbatch-proxy`. The user submits job requests to `kbatch-proxy` and `kbatch-proxy` submits those job requests to the Kubernetes API. \n",
    "\n",
    "> NOTE:\n",
    "> At the present, no additional features have been added to `kbatch`, we have simply integrated `kbatch-proxy` into QHub. A feature enhancement PR will need to be opened on the `kbatch` repo in order to enable cronjobs.\n",
    "\n",
    "### Setup\n",
    "\n",
    "In order to use `kbatch` in it's current form, some basic setup is required of the user. Going forward, we will assume that `kbatch-proxy` has been correctly integrated into QHub.\n",
    "\n",
    "1. Create or modify a conda environment by adding `kbatch`. And activate this conda environment.\n",
    "\n",
    "```\n",
    "pip install kbatch\n",
    "```\n",
    "\n",
    "2. Create a JupyterHub API token and configure `kbatch` to talk to the `kbatch-proxy` service.\n",
    "\n",
    "```\n",
    "kbatch configure --kbatch-url http://kbatch-kbatch-proxy.dev.svc.cluster.local --token <JUPYTERHUB_API_TOKEN>\n",
    "```\n",
    "\n",
    "3. Submit a job to `kbatch`\n",
    "\n",
    "```\n",
    "kbatch job submit --name=list-files --image=alpine --command='[\"ls\", \"-lh\"]'\n",
    "```\n",
    "\n",
    "### Run this notebook\n",
    "\n",
    "To run this notebook as a job, you will need an image with `papermill` (or a similar CLI tool). \n",
    "\n",
    "Create a configuration file, `kbatch_nb_job.yaml` like the one below:\n",
    "```yaml\n",
    "# filename: kbatch_nb_job.yaml\n",
    "name: nb-job\n",
    "image: mcr.microsoft.com/planetary-computer/python:latest\n",
    "args:\n",
    "  - papermill\n",
    "  - kbatch_nb.ipynb\n",
    "code: kbatch_nb.ipynb\n",
    "```\n",
    "\n",
    "Then run:\n",
    "```\n",
    "kbatch job submit -f kbatch_nb_job.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3e90f6-f0b6-4300-81c2-b7ade75d57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "277ff7d7-d938-4790-af92-5719534b08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kbatch_nb_output.txt', 'w') as f:\n",
    "    for i in range(0,10):\n",
    "        current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "        time.sleep(1)\n",
    "        f.write(f'{current_time}: {i}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
